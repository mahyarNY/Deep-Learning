{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Fully Connected Neural Networks\n",
    "\n",
    "We covered artificial neural networks with multiple hidden layers in class. In this assignment, you will implement Fully Connected Neural Network (FCN) components in order to perform a supervised classification task.\n",
    "\n",
    "The dataset you are going to work with are : (i) for development of your code, you will use Wine dataset for classification; (ii) for actual training and testing of your implementation in this assignment, the actual dataset will be Book Genre Classification data. You will be performing a genre classification of books into 32 categories.\n",
    "\n",
    "Usage of any built-in functions for code parts that you are asked to write are not allowed. We provide a skeleton code on which to build on your own architecture. In the Layer class, there are two important methods, named as forward and backward. Almost everything you will use in this assignment is derived from this class. We will follow PyTorch-like architecture in the skeleton code.\n",
    "\n",
    "**Please do not modify the following cells, except the book genre classification cell. We will use them for the evaluation of your homeworks. **\n",
    "\n",
    "**You should modify and fill in the code under blg561/layers.py, which includes functions such as layer.NNLayer.* ...**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from blg561e.layer import layer\n",
    "from blg561e.checks import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To auto-reload your modules from the *.py files, re run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In the `Layer` class, there are two important methods, named as `forward` and `backward`. Almost everything you will use in this assignment is derived from this class. You will be programming in Python language.\n",
    "\n",
    "**Don't forget to test your implementation by using the cells below!**\n",
    "\n",
    "\n",
    "\n",
    "### a. Affine Layer\n",
    "\n",
    "In this layer, we basically implement the hidden layers of neural nets. Each neuron (building block of neural networks) is a just logistic regression classifier itself, but stacking these neurons make them powerful to implement any function.\n",
    "We are going to implement our affine layer \n",
    "\n",
    "Go under blg561e/layer.py and find Affine class. Implement the forward pass for Affine layer which is formulated as follows:\n",
    "\n",
    "$ z = W x + b $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of affine layer:\n",
      "difference:  8.825372662436368e-08\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 10\n",
    "input_shape = (4, 7, 2) \n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)#560\n",
    "weight_size = output_dim * np.prod(input_shape)#168\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "affineLayer.W = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "affineLayer.b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out = affineLayer.forward(x)\n",
    "correct_out = np.array([[-0.34448963, -0.15630714,  0.03187535],\n",
    "       [-0.18626697,  0.0119934 ,  0.21025377],\n",
    "       [-0.0280443 ,  0.18029394,  0.38863218],\n",
    "       [ 0.13017836,  0.34859447,  0.56701059],\n",
    "       [ 0.28840102,  0.51689501,  0.74538901],\n",
    "       [ 0.44662368,  0.68519555,  0.92376742],\n",
    "       [ 0.60484634,  0.85349608,  1.10214583],\n",
    "       [ 0.763069  ,  1.02179662,  1.28052425],\n",
    "       [ 0.92129166,  1.19009716,  1.45890266],\n",
    "       [ 1.07951432,  1.35839769,  1.63728107]])\n",
    "\n",
    "\n",
    "relError = rel_error(out, correct_out)\n",
    "\n",
    "print('Testing forward method of affine layer:')\n",
    "print('difference: ', relError)\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass : \n",
    "Go under blg561e/layer.py and find AffineLayer class. Implement the backward pass for Affine layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of affine layer:\n",
      "dx error:  7.882509889959262e-10\n",
      "dw error:  1.3592685518020832e-10\n",
      "db error:  1.8477112902497496e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_inputs = 7\n",
    "input_shape = (4, 10, 3)\n",
    "output_dim = 8\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "affineLayer.W = np.random.randn(6, 5)\n",
    "affineLayer.b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = grad_check(affineLayer.forward, x, dout)\n",
    "dw_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.W, dout)\n",
    "db_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.b, dout)\n",
    "\n",
    "affineLayer.forward(x)\n",
    "dx, dw, db = affineLayer.backward(dout)\n",
    "\n",
    "# Errors should be around 1e-6 at least\n",
    "print('Testing backward method of affine layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "\n",
    "assert 1e-6 > rel_error(dx_num, dx) \n",
    "assert 1e-6 > rel_error(dw_num, dw) \n",
    "assert 1e-6 > rel_error(db_num, db) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b. ReLU Layer\n",
    "\n",
    "Go under `blg561e/layer.py` and find `ReLU` class. Implement the forward pass for ReLU which is basicly zeroing the negative inputs:\n",
    "\n",
    "$ ReLU(x) = max(x, 0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of ReLU layer:\n",
      "Error:  0.0\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "x = np.array([0,1,3,4,-1,2,4,1773,-1773, 1.3, .4, -.1]).reshape(3, -1)\n",
    "out = relu.forward(x)\n",
    "correct_out = np.array([[0.000, 1.000, 3.000, 4.000],\n",
    "                       [0.000, 2.000, 4.000, 1773],\n",
    "                       [0.000, 1.300, 0.4, 0]])\n",
    "\n",
    "# Compare your output with ours. \n",
    "relError = rel_error(out, correct_out)\n",
    "print('Testing forward method of ReLU layer:')\n",
    "print('Error: ', rel_error(out, correct_out))\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of ReLU layer:\n",
      "dx error:  3.2756263483625388e-12\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(relu.forward, x, dout)\n",
    "\n",
    "relu.forward(x)\n",
    "dx = relu.backward(dout)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing backward method of ReLU layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Softmax classifier \n",
    "\n",
    "In multi-class classification task, as we've seen in the class, the softmax loss function is utilized. \n",
    "Practically, at the final layer of the network, instead of the standard activation, we utilize softmax function to turn the likelihood of each class into class probabilities. Then, we utilize the cross-entropy loss as the data loss. Below, you implement and return only the data loss component in your overall loss. \n",
    "*** Implement your loss computation in the function \"loss\" of the layer.py ***\n",
    "\n",
    "The L2 regularizer will be added by you in the Optimization phase later.\n",
    "You will write forward pass and backward pass for the softmax unit. Below, we evaluate your method by a numerical gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302478992941877\n",
      "dx error:  9.563859596159055e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "softmax = layer.Softmax()\n",
    "\n",
    "def softmax_loss (x,y):\n",
    "    probs = softmax.forward(x)\n",
    "    dx = softmax.backward(y,)\n",
    "    loss = layer.loss(probs, y) \n",
    "#     print(-np.sum(probs[np.arange(y.shape[0]), y]) / y.shape[0])\n",
    "    return loss,dx\n",
    "\n",
    "\n",
    "loss, dx = softmax_loss(x,y)\n",
    "dx_num = grad_check(lambda x: softmax_loss(x, y)[0], x)\n",
    "\n",
    "# The loss should be about 2.3\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Implement your activation (Bonus)\n",
    "Implement a novel or a recently published activation function and test its correctness below. If you used an activation from a paper, please don't forget to give a reference to it. Make sure that you have the correct implementation of the forward pass so that we can test your backward pass using a numerical gradient.\n",
    "\n",
    "Also, under this cell, write your activation mathematically and its derivative. Do not forget to use your activation in training part with the Wine data to show that it works and makes sense. You can also plot your activation for litte extra credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your activation:\n",
      "dx error:  5.469181638412024e-12\n"
     ]
    }
   ],
   "source": [
    "act = layer.YourActivation() #quadratic activation function\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(act.forward, x, dout)\n",
    "\n",
    "act.forward(x)\n",
    "dx = act.backward(dout)\n",
    "\n",
    "relError = rel_error(dx_num, dx)\n",
    "print('Testing your activation:')\n",
    "print('dx error: ', relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Optimizers\n",
    "\n",
    "Implement SGD and SGDWithMomentum Strategies in `VanillaSGDOptimizer` and `SGDWithMomentum` classes. Test their correctness using cell below. \n",
    "**Do not forget to add L2 regularization to both optimizers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.VanillaSDGOptimizer(model=toyModel, lr=1, regularization_str=1e-1)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [ np.array([[ 0.97873084,  0.81250429],\n",
    " [-3.7373582,  -4.06007668],\n",
    " [ 0.29461562, -0.37317717],\n",
    " [ 0.23786611 , 0.27586238],\n",
    " [-1.45262147, -2.34007449],\n",
    " [ 0.03742712, -0.24127232],\n",
    " [ 0.2617457 ,  0.51694319],\n",
    " [ 0.35243035,  0.96434886],\n",
    " [ 0.17950643,  0.76174137],\n",
    " [ 1.62739663,  1.42935729]]),\n",
    "np.array([-0.23634795, -0.22072128]),\n",
    "np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    " [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    "np.array([-0.34588157, -0.00713497,  0.35301654])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n",
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n"
     ]
    }
   ],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append( toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.SGDWithMomentum(model=toyModel, lr=1, regularization_str=1e-1, mu=.5)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [np.array([[ 0.97873084,  0.81250429],\n",
    "        [-3.7373582 , -4.06007668],\n",
    "        [ 0.29461562, -0.37317717],\n",
    "        [ 0.23786611,  0.27586238],\n",
    "        [-1.45262147, -2.34007449],\n",
    "        [ 0.03742712, -0.24127232],\n",
    "        [ 0.2617457 ,  0.51694319],\n",
    "        [ 0.35243035,  0.96434886],\n",
    "        [ 0.17950643,  0.76174137],\n",
    "        [ 1.62739663,  1.42935729]]),\n",
    " np.array([-0.23634795, -0.22072128]),\n",
    " np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    "        [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    " np.array([-0.34588157, -0.00713497,  0.35301654])]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n",
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n"
     ]
    }
   ],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append( toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Build your own model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example which is implemented using previously defined API. In this example, you will use the widely known Wine dataset (https://archive.ics.uci.edu/ml/datasets/wine). Each instance has 13 features as the chemical analysis of wines and you will classify the data where the class number is 3 and each class represents different origin of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_wine  # Load dataset\n",
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names) # Before training, understand your data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.898093375027604, Accuracy: 0.14788732394366197\n",
      "Epoch: 0, Test Loss: 2.417699726033659, Test Accuracy: 0.2777777777777778\n",
      "Epoch: 50, Loss: 0.41389596352851854, Accuracy: 0.7816901408450704\n",
      "Epoch: 100, Loss: 0.2949131835037399, Accuracy: 0.8732394366197183\n",
      "Epoch: 150, Loss: 0.21462377388739715, Accuracy: 0.9014084507042254\n",
      "Epoch: 200, Loss: 0.16584438266289572, Accuracy: 0.9295774647887324\n",
      "Epoch: 200, Test Loss: 0.2180630432373385, Test Accuracy: 0.8888888888888888\n",
      "Epoch: 250, Loss: 0.13405605326481546, Accuracy: 0.9366197183098591\n",
      "Epoch: 300, Loss: 0.11188668090204187, Accuracy: 0.9436619718309859\n",
      "Epoch: 350, Loss: 0.08717145580020876, Accuracy: 0.9647887323943662\n",
      "Epoch: 400, Loss: 0.07294023874753161, Accuracy: 0.971830985915493\n",
      "Epoch: 400, Test Loss: 0.08134230270236939, Test Accuracy: 0.9722222222222222\n",
      "Epoch: 450, Loss: 0.06333505231855464, Accuracy: 0.971830985915493\n",
      "Epoch: 500, Loss: 0.05665640270452128, Accuracy: 0.971830985915493\n",
      "Epoch: 550, Loss: 0.04786095483911245, Accuracy: 0.9859154929577465\n",
      "Epoch: 600, Loss: 0.038199063549694136, Accuracy: 0.9859154929577465\n",
      "Epoch: 600, Test Loss: 0.06296957813350171, Test Accuracy: 0.9722222222222222\n",
      "Epoch: 650, Loss: 0.03264938763177443, Accuracy: 0.9859154929577465\n",
      "Epoch: 700, Loss: 0.029206731395120592, Accuracy: 0.9859154929577465\n",
      "Epoch: 750, Loss: 0.02687280576331791, Accuracy: 0.9859154929577465\n",
      "Epoch: 800, Loss: 0.025168805181517217, Accuracy: 0.9859154929577465\n",
      "Epoch: 800, Test Loss: 0.05874511064550041, Test Accuracy: 0.9722222222222222\n",
      "Epoch: 850, Loss: 0.023884205477080995, Accuracy: 0.9859154929577465\n",
      "Epoch: 900, Loss: 0.022879324626576433, Accuracy: 0.9859154929577465\n",
      "Epoch: 950, Loss: 0.02206700961460787, Accuracy: 0.9859154929577465\n",
      "Epoch: 1000, Loss: 0.021406578416769646, Accuracy: 0.9859154929577465\n",
      "Epoch: 1000, Test Loss: 0.05754169786116224, Test Accuracy: 0.9722222222222222\n",
      "Epoch: 1050, Loss: 0.02072874442792498, Accuracy: 0.9859154929577465\n",
      "Epoch: 1100, Loss: 0.02019320305381045, Accuracy: 0.9859154929577465\n",
      "Epoch: 1150, Loss: 0.01975544345335635, Accuracy: 0.9859154929577465\n"
     ]
    }
   ],
   "source": [
    "X, y = data.data, data.target # Get the features and the corresponding classes\n",
    "model = layer.Model() # Create a model instance\n",
    " \n",
    "# Wine dataset has 13 features, so the input size of first layer is 13. We have 3 classes, so size of last hidden is 3. \n",
    "# Each neuron corresponds the likelihood of a class, named P(y=neuron_index|x), where y is class label \n",
    "# and x is features given.\n",
    "layers = [layer.AffineLayer(13,64), layer.ReLU(), layer.AffineLayer(64,3), layer.Softmax()]\n",
    "\n",
    "model(layers) # Load layers to model object\n",
    "predictions  = np.ones(178) # Number of instances in the Wine data is 178\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Shuffle dataset\n",
    "def create_permutation(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    return x[perm], y[perm]\n",
    "\n",
    "def train_test_split(X, y, ratio=.2):\n",
    "    X, y = create_permutation(X, y)\n",
    "    split_index =  int(len(X) * (1-ratio))\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_test, y_test = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "# Options\n",
    "preprocessing_on = True\n",
    "shuffle_on_each_epoch = True\n",
    "regularization_strength = 0\n",
    "n_epochs = 1200\n",
    "train_test_split_ratio = .2\n",
    "print_every = 50\n",
    "test_every = 200\n",
    "if preprocessing_on:\n",
    "    X = preprocessing.scale(X)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "optimizer = layer.SGDWithMomentum(model,lr=1e-1, regularization_str=regularization_strength)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if shuffle_on_each_epoch:\n",
    "        X_train, y_train = create_permutation(X_train, y_train)\n",
    "    softmax_out = model.forward(X_train)\n",
    "\n",
    "    predictions = np.argmax(softmax_out, axis=1)\n",
    "    train_acc = np.mean(predictions == y_train)\n",
    "    loss = layer.loss(softmax_out, y_train)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "    \n",
    "    model.backward(y_train)\n",
    "    optimizer.optimize()\n",
    "    \n",
    "    if epoch % test_every == 0:\n",
    "        softmax_out = model.forward(X_test)\n",
    "        predictions = np.argmax(softmax_out, axis=1)\n",
    "        loss = layer.loss(softmax_out, y_test)\n",
    "        test_acc = np.mean(predictions == y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_accs.append([test_acc for i in range(test_every)])\n",
    "        print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Plot the training and test loss curves for diagnostics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255ff9e98c8>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(n_epochs)\n",
    "plt.plot(epochs, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, test_losses, 'b', label='testing loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('test loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.plot(test_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss curve shows that the training converge very fast and shows overfitting of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II - Book Genre Classification\n",
    "\n",
    "Now, in this part, you will work with text data (https://arxiv.org/pdf/1610.09204.pdf) for book genre analysis. Originally, the dataset is used for book genre classification by the book cover image. In this part, you will classify the books into their genres by their titles. The total number of genres for the books to be classified into is 32.\n",
    "\n",
    "Below, we already implemented the preprocessing codes fro the data. Run the below cells and load the text data \"book32-listing.csv\" into an appropriate form. You will need to use batch-wise optimizer since it is almost impossible to fit all the data at once.\n",
    "\n",
    "**IMPORTANT: You are NOT allowed to use sklearn or any other implementations for the learning part\n",
    ". You are ALLOWED ONLY TO USE your own implementation from the above steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Image_link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Class</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>761183272</td>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1623439671</td>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B00O80WC6I</td>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761182187</td>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578052084</td>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Image                                         Image_link  \\\n",
       "Id                                                                              \n",
       "761183272   0761183272.jpg  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1623439671  1623439671.jpg  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "B00O80WC6I  B00O80WC6I.jpg  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "761182187   0761182187.jpg  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "1578052084  1578052084.jpg  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                                        Title  \\\n",
       "Id                                                              \n",
       "761183272                     Mom's Family Wall Calendar 2016   \n",
       "1623439671                    Doug the Pug 2016 Wall Calendar   \n",
       "B00O80WC6I  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...   \n",
       "761182187             365 Cats Color Page-A-Day Calendar 2016   \n",
       "1578052084               Sierra Club Engagement Calendar 2016   \n",
       "\n",
       "                        Author  Class      Genre  \n",
       "Id                                                \n",
       "761183272       Sandra Boynton      3  Calendars  \n",
       "1623439671        Doug the Pug      3  Calendars  \n",
       "B00O80WC6I           Moleskine      3  Calendars  \n",
       "761182187   Workman Publishing      3  Calendars  \n",
       "1578052084         Sierra Club      3  Calendars  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv into a data frame\n",
    "csv = 'book32-listing.csv'\n",
    "all_data = pd.read_csv(csv, encoding = 'ISO-8859-1', index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahyar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>471839655</td>\n",
       "      <td>Fundamentals of Photonics (Wiley Series in Pur...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604691956</td>\n",
       "      <td>50 Beautiful Deer-Resistant Plants: The Pretti...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62237330</td>\n",
       "      <td>Eric: A Novel of Discworld Terry Pratchett</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472051849</td>\n",
       "      <td>The North Country Trail: The Best Walks, Hikes...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806983590</td>\n",
       "      <td>The Rug Hook Book: Techniques, Projects And Pa...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Text  Class\n",
       "Id                                                                  \n",
       "471839655   Fundamentals of Photonics (Wiley Series in Pur...     23\n",
       "1604691956  50 Beautiful Deer-Resistant Plants: The Pretti...      8\n",
       "62237330           Eric: A Novel of Discworld Terry Pratchett     24\n",
       "472051849   The North Country Trail: The Best Walks, Hikes...     29\n",
       "806983590   The Rug Hook Book: Techniques, Projects And Pa...      8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we only care about the Title, Author and Class columns, we will extract them and shuffle the data\n",
    "# We can enrich the feature representation by including the Author information\n",
    "from sklearn.utils import shuffle\n",
    "data = all_data[['Title', 'Author', 'Class']]\n",
    "data['Text'] = data['Title'].astype(str) + ' ' + data['Author'].astype(str)\n",
    "data = data[['Text', 'Class']]\n",
    "data = shuffle(data, random_state=42)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>471839655</td>\n",
       "      <td>fundamentals photonics wiley series pure appli...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604691956</td>\n",
       "      <td>beautiful deer resistant plants prettiest annu...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62237330</td>\n",
       "      <td>eric novel discworld terry pratchett</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472051849</td>\n",
       "      <td>north country trail best walks hikes backpacki...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806983590</td>\n",
       "      <td>rug hook book techniques projects patterns eas...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Text  Class\n",
       "Id                                                                  \n",
       "471839655   fundamentals photonics wiley series pure appli...     23\n",
       "1604691956  beautiful deer resistant plants prettiest annu...      8\n",
       "62237330                 eric novel discworld terry pratchett     24\n",
       "472051849   north country trail best walks hikes backpacki...     29\n",
       "806983590   rug hook book techniques projects patterns eas...      8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we will use some very basic text cleaning steps \n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords') # After you download the data, you can comment this line \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Stopwords carry far less meaning than other keywords in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # Remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # Remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # Convert text to lowercase \n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 500)\n"
     ]
    }
   ],
   "source": [
    "# We will extract features from the text and split the data into training, validation and test sets\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=500) \n",
    "# You can change the max_features if you encounter a memory error, but do not make it too small\n",
    "\n",
    "x_train_series, y_train = data['Text'][:150000], data['Class'][:150000] # 150K train\n",
    "# print(x_train_series.shape, y_train.shape)\n",
    "x_val_series, y_val = data['Text'][150000:180000], data['Class'][150000:180000] # 30K val\n",
    "x_test_series, y_test = data['Text'][180000:], data['Class'][180000:] # ~30K test\n",
    "\n",
    "x_train = np.array(vectorizer.fit_transform(x_train_series).todense())\n",
    "print(x_train.shape)\n",
    "x_val = np.array(vectorizer.transform(x_val_series).todense())\n",
    "x_test = np.array(vectorizer.transform(x_test_series).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. You will use your implementations (layers.py) below to carry out the book genre classification. Construct your model with all its layers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = x_train, y_train # Get the features and the corresponding classes of the book genre classification\n",
    "model = layer.Model() # Create a model instance\n",
    " \n",
    "# Wine dataset has 2500 features but I choose to use only 500 features, so the input size of first layer is 500. We have 32 classes, so size of last hidden is 32. \n",
    "# Each neuron corresponds the likelihood of a class, named P(y=neuron_index|x), where y is class label \n",
    "# and x is features given.\n",
    "layers = [layer.AffineLayer(500,750), layer.ReLU(), layer.AffineLayer(750,32), layer.Softmax()]\n",
    "\n",
    "model(layers) # Load layers to model object\n",
    "predictions  = np.ones(12000) # Number of instances in the genre book training data is 12000\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Shuffle dataset\n",
    "def create_permutation(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    return x[perm], y[perm]\n",
    "\n",
    "def train_test_split(X, y, ratio=.2):\n",
    "    X, y = create_permutation(X, y)\n",
    "    split_index =  int(len(X) * (1-ratio))\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_test, y_test = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "# Options\n",
    "preprocessing_on = True\n",
    "shuffle_on_each_epoch = True\n",
    "regularization_strength = 0\n",
    "n_epochs = 1200\n",
    "train_test_split_ratio = .2\n",
    "print_every = 50\n",
    "test_every = 200\n",
    "if preprocessing_on:\n",
    "    X = preprocessing.scale(X)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = layer.SGDWithMomentum(model,lr=1e-1, regularization_str=regularization_strength)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if shuffle_on_each_epoch:\n",
    "        X_train, y_train = create_permutation(X_train, y_train)\n",
    "    softmax_out = model.forward(X_train)\n",
    "\n",
    "    predictions = np.argmax(softmax_out, axis=1)\n",
    "    train_acc = np.mean(predictions == y_train)\n",
    "    loss = layer.loss(softmax_out, y_train)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "    \n",
    "    model.backward(y_train)\n",
    "    optimizer.optimize()\n",
    "    \n",
    "    if epoch % test_every == 0:\n",
    "        softmax_out = model.forward(X_test)\n",
    "        predictions = np.argmax(softmax_out, axis=1)\n",
    "        loss = layer.loss(softmax_out, y_test)\n",
    "        test_acc = np.mean(predictions == y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_accs.append([test_acc for i in range(test_every)])\n",
    "        print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Plot histogram of the weights of affine layers to see whether the weights vanish or not and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = range(n_epochs)\n",
    "plt.plot(epochs, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, test_losses, 'b', label='testing loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('test loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.plot(test_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run diagnostics of your model : Try different hyperparameter settings such as number of layers in your model, learning rate, regularization parameter and such.  Avoid overfitting and underfitting as much as possible. We expect you to get at least 50% test accuracy with your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to improve the model performance we can improve hyperparameter setting by using first generating random hyperparameter\n",
    "#By this approach we randomly determine max and min of the desired hyperparameter, so that the search space to find the best\n",
    "#one will decrease.\n",
    "\n",
    "# Generate random hyperparameters given ranges for each of them\n",
    "def generate_random_hyperparams(lr_min, lr_max, reg_min, reg_max):\n",
    "    lr = 10**np.random.uniform(lr_min,lr_max)\n",
    "    reg = 10**np.random.uniform(reg_min,reg_max)\n",
    "    #we can even consider the numbers of hidden layer as a hyperparameter, but for the sake of simplicity we don't do this\n",
    "#     hidden = np.random.randint(h_min, h_max)\n",
    "    return lr, reg\n",
    "\n",
    "#Now that we have a smaller range of hyperparameter, we iterate through the training process and change the lr and reg in \n",
    "#each iteration. when the performance is better than the previous accuracy that we had achieved, we can choose it as our\n",
    "#updated hyperparameters\n",
    "\n",
    "for i in range(20):\n",
    "    lr, reg = generate_random_hyperparams(-4,-2, -7, -4)\n",
    "    model = layer.model()\n",
    "    layers = [layer.AffineLayer(500,750), layer.ReLU(), layer.AffineLayer(750,32), layer.Softmax()]\n",
    "    model(layers) # Load layers to model object\n",
    "    predictions  = np.ones(12000) \n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    # Options\n",
    "    preprocessing_on = True\n",
    "    shuffle_on_each_epoch = True\n",
    "    regularization_strength = reg\n",
    "    n_epochs = 1200\n",
    "    train_test_split_ratio = .2\n",
    "    print_every = 50\n",
    "    test_every = 200\n",
    "    if preprocessing_on:\n",
    "        X = preprocessing.scale(X)\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    " #*******************************************************************************************************   \n",
    "    #hyperparameters update each iteration\n",
    "    optimizer = layer.SGDWithMomentum(model,lr=lr, regularization_str=regularization_strength)\n",
    "    for epoch in range(n_epochs):\n",
    "        if shuffle_on_each_epoch:\n",
    "            X_train, y_train = create_permutation(X_train, y_train)\n",
    "        softmax_out = model.forward(X_train)\n",
    "\n",
    "        predictions = np.argmax(softmax_out, axis=1)\n",
    "        train_acc = np.mean(predictions == y_train)\n",
    "        loss = layer.loss(softmax_out, y_train)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "\n",
    "        model.backward(y_train)\n",
    "        optimizer.optimize()\n",
    "\n",
    "        if epoch % test_every == 0:\n",
    "            softmax_out = model.forward(X_test)\n",
    "            predictions = np.argmax(softmax_out, axis=1)\n",
    "            loss = layer.loss(softmax_out, y_test)\n",
    "            test_acc = np.mean(predictions == y_test)\n",
    "            test_losses.append(loss)\n",
    "            test_accs.append([test_acc for i in range(test_every)])\n",
    "            print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, loss, test_acc))\n",
    "    \n",
    "        val_accuracy = test_acc\n",
    "        if best_val < val_accuracy:\n",
    "            best_val = val_accuracy\n",
    "#************************************************************************************************\n",
    "    # Print results\n",
    "    print('lr %e reg %e hid %d  val accuracy: %f' % (\n",
    "                lr, reg, hidden_size, val_accuracy))\n",
    "print('best validation accuracy achieved: %f' % best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Plot the training and validation losses versus number of iterations, as you vary the regularization parameter lambda with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part after getting the updated regularization parameter training part, we can visualized the training and \n",
    "#validation loss to check if the method we have used, has improved the performance of the model or not\n",
    "\n",
    "epochs = range(n_epochs)\n",
    "plt.plot(epochs, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, test_losses, 'b', label='testing loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('test loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot the training and validation losses as you vary the Learning Parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part after getting the updated learning parameter alpha training part, we can visualized the training and \n",
    "#validation loss to check if the method we have used, has improved the performance of the model or not. the updating method\n",
    "#actually help us to check if we have vanishing gradient or not and if so, solve the issue by increasing the learning rate\n",
    "\n",
    "epochs = range(n_epochs)\n",
    "plt.plot(epochs, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, test_losses, 'b', label='testing loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('test loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use two different optimizers: Mini-batch SGD and Mini-batch SGD with Momentum, and plot training and validation losses versus Iteration numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this part we can choose to use SGD with momentum or vanilla SGD and after training check which one perform better by \n",
    "#plotting training and validation losses\n",
    "\n",
    "optimizer1 = layer.VanillaSDGOptimizer(model,lr=lr, regularization_str=regularization_strength)\n",
    "optimizer2 = layer.SGDWithMomentum(model,lr=lr, regularization_str=regularization_strength)\n",
    "\n",
    "#Because I encountered an error in previous parts I can't show the results but we can seperately train the model with optimizer1\n",
    "#and optimizer2, then plot the outputs to evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Finally, fix your model and hyperparameters according to your observations above. Plot accuracy of your classification for training and validation sets, and print your test accuracy. Remember that the test accuracy shoud be at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
